{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "def save_text_to_file(part, content, index):\n",
    "    filename = f\"{part}_{index}.txt\"\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "    print(f\"Saved content to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 1 ###\n",
    "\n",
    "seed_url = \"https://press.un.org/en\"\n",
    "\n",
    "urls = [seed_url]\n",
    "seen = [seed_url]\n",
    "opened = []\n",
    "press_releases = []\n",
    "\n",
    "maxNumUrl = 1000  # increased the maximum number as 50 might not be enough to find 10 press releases with \"crisis\"\n",
    "\n",
    "while len(urls) > 0 and len(press_releases) < 10 and len(opened) < maxNumUrl:\n",
    "    try:\n",
    "        curr_url = urls.pop(0)\n",
    "        req = urllib.request.Request(curr_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urllib.request.urlopen(req).read()\n",
    "        opened.append(curr_url)\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(\"Unable to access= \" + curr_url)\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(webpage, 'html.parser')\n",
    "\n",
    "    # Check if it's a press release\n",
    "    press_release_tag = soup.find('a', {'href': '/en/press-release', 'hreflang': 'en'})\n",
    "    if press_release_tag:\n",
    "        content = soup.prettify()\n",
    "        if \"crisis\" in content.lower():  # check if content contains the word 'crisis'\n",
    "            press_releases.append(curr_url)\n",
    "            save_text_to_file(1, content, len(press_releases))\n",
    "            print(\"Found a press release with 'crisis' in it: \" + curr_url)\n",
    "            continue  # Skip adding child urls if this is a press release\n",
    "\n",
    "    for tag in soup.find_all('a', href=True):\n",
    "                childUrl = tag['href']\n",
    "                childUrl = urllib.parse.urljoin(seed_url, childUrl)\n",
    "\n",
    "                if childUrl not in seen:\n",
    "                    urls.append(childUrl)\n",
    "                    seen.append(childUrl)\n",
    "\n",
    "print(\"Number of press releases containing 'crisis':\", len(press_releases))\n",
    "print(\"Press releases URLs:\")\n",
    "for pr_url in press_releases:\n",
    "    print(pr_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 2 ###\n",
    "\n",
    "\n",
    "urls = [seed_url]\n",
    "seen = [seed_url]\n",
    "opened = []\n",
    "press_releases = []\n",
    "\n",
    "maxNumUrl = 1000  # Increase the maximum as 50 might not be enough to find 10 press releases with \"crisis\".\n",
    "\n",
    "# Iterate over pages from 0 to 10\n",
    "for page_number in range(11):\n",
    "    # Exit if we've reached the desired number of press releases\n",
    "    print(page_number)\n",
    "\n",
    "    seed_url = f\"https://www.europarl.europa.eu/news/en/press-room/page/{page_number}\"\n",
    "    urls = [seed_url]\n",
    "    seen = [seed_url]\n",
    "\n",
    "    while len(urls) > 0 and len(press_releases) < 10 and len(opened) < maxNumUrl:\n",
    "        try:\n",
    "            curr_url = urls.pop(0)\n",
    "            req = urllib.request.Request(curr_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            webpage = urllib.request.urlopen(req).read()\n",
    "            opened.append(curr_url)\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(\"Unable to access= \" + curr_url)\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(webpage, 'html.parser')\n",
    "\n",
    "        # Check if it's a press release related to plenary sessions\n",
    "        plenary_tag = soup.find('span', {'class': 'ep_name'}, string='Plenary session')\n",
    "        if plenary_tag:          \n",
    "            # Get the HTML source code\n",
    "            content = soup.prettify()\n",
    "\n",
    "            if \"crisis\" in content.lower() and curr_url.startswith('https://www.europarl.europa.eu/news/en/press-room/20'):  # check if content contains the word 'crisis'\n",
    "                press_releases.append(curr_url)\n",
    "                print(\"Found a press release with 'crisis' in it: \" + curr_url)\n",
    "                save_text_to_file(2, content, len(press_releases))\n",
    "                continue  # Skip adding child urls if this is a press release\n",
    "\n",
    "        for tag in soup.find_all('a', href=True):\n",
    "            if tag['href'].startswith('https://www.europarl.europa.eu/news/en/press-room/20'):\n",
    "                childUrl = tag['href']\n",
    "                childUrl = urllib.parse.urljoin(seed_url, childUrl)\n",
    "\n",
    "                if childUrl not in seen:\n",
    "                    urls.append(childUrl)\n",
    "                    seen.append(childUrl)\n",
    "\n",
    "print(\"Number of press releases about plenary sessions containing 'crisis':\", len(press_releases))\n",
    "print(\"Press releases URLs:\")\n",
    "for pr_url in press_releases:\n",
    "    print(pr_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
